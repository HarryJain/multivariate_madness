---
title: "Cluster Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loaddata, include = FALSE}
source("load_data.R")
```

As our final multivariate method, let's apply cluster analysis to our data. This will allow us to use our subset of variables to find clusters of teams in multidimensional space. Then, we can analyze these clusters for common characteristics and potential predictions of tournament success.

## Hierarchial Cluster Analysis
To start, let's complete some hierarchial cluster analyses with various distance and agglomeration methods. To do so, we create a generalized data set and function, try several different metric combinations, and analyze the results, choosing an appropriate number of clusters and considering their meaning.

```{r, include = FALSE, message = FALSE}
# Make a vector of all variables used in cluster analysis and the interpretations
# (a few representative regular season offensive and defensive statistics
# as well as basic tournament statistics)
vars.cluster <- c("name", "year", "seed", "points", "rebounds", "assists", "steals", "blocks",
          "turnovers", "personal_fouls", "true_shooting_pct", "points_in_paint",
          "fast_break_pts", "points_off_turnovers", "efficiency", "opp_points",
          "opp_rebounds", "opp_assists", "opp_turnovers", "opp_personal_fouls",
          "opp_true_shooting_pct", "opp_efficiency", "tourn_games_played",
          "awp", "owp", "sos", "rpi")

# Make a vector of response variables used in cluster analysis (a few representative
#   regular season statistics for both offense and defense)
vars.cluster.response <- c("points", "rebounds", "assists", "steals", "blocks", 
                  "turnovers", "personal_fouls", "true_shooting_pct", "points_in_paint", 
                  "fast_break_pts", "points_off_turnovers", "efficiency", "opp_points", 
                  "opp_rebounds", "opp_assists", "opp_turnovers", "opp_personal_fouls",
                  "opp_true_shooting_pct", "opp_efficiency", "awp", "owp", "sos", "rpi")


# Get a subset of the data based on the chosen variables
TD.cluster <- TD[, vars.cluster]

# Remove any teams with incomplete data (some of the older tournament teams lack 
#   more composite statistics)
TD.cluster <- TD.cluster[complete.cases(TD.cluster), ]

# Output the number of rows and columns
dim(TD.cluster)
# List the variables chosen
names(TD.cluster)

# Create standardized data subset including only the 2018-19 Tournament Teams
TD.cluster.norm <- scale(na.omit(TD.cluster[which(TD$year == "2018"), vars.cluster.response]))
rownames(TD.cluster.norm) <- TD.cluster[which(TD$year == "2018"), 1]

# Define function for different clustering methods
cluster = function(distance, clustering, clusters)
{
  # Get the distance matrix using Euclidean distance
  dist1 <- dist(TD.cluster.norm, method = distance)
  
  # Do clustering using the distance matrix
  clust1 <- hclust(dist1, method = clustering)
  
  # Draw the dendrogram with 3 clusters
  plot(clust1, labels = rownames(TD.cluster.norm), cex = 0.4, xlab = "", ylab = "Distance", 
       main = "Clustering of Teams")
  rect.hclust(clust1, k = clusters)
  
  # Create a vector for which teams are in what group
  cuts <- cutree(clust1, k = clusters)
  
  rownames(TD.cluster.norm) <- paste(TD.cluster[which(TD$year == "2018"), 1], 
                            "Seed:", TD.cluster[which(TD$year == "2018"), "seed"], 
                            "Tourn Games Played:", TD.cluster[which(TD$year == "2018"), 
                                                       "tourn_games_played"])
  
  team.clusters = list()
  
  # Print a list of teams in each group, along with their seed 
  #   and number of tournament games played
  for (i in 1:clusters)
  {
    team.clusters[[i]] <- rownames(TD.cluster.norm)[cuts == i]
    print(paste("Teams in Cluster ", i))
    print(rownames(TD.cluster.norm)[cuts == i])
    print (" ")
  }
  
  test <- as.data.frame(sapply(team.clusters, "length<-", max(lengths(team.clusters))))
  #pander(test, justify = "left")
  
  # Reset rownames to just the team names
  rownames(TD.cluster.norm) <- TD.cluster[which(TD.cluster$year == "2018"), 1]
  
  # Get functions for "number of clusters" metrics
  source("http://reuningscherer.net/stat660/R/HClusEval.R.txt")
  # Call the function to analyze the number of clusters to use
  hclus_eval(TD.cluster.norm, dist_m = distance, clus_m = clustering, plot_op = T)
  
  # Make a plot of the cluster solution in the space specified by 
  #   the first two principal components
  clusplot(TD.cluster.norm, cuts, color = TRUE, shade = TRUE, labels = 2, lines = 0, cex = 0.5,
         main = paste("2018 NCAA Tournament Cluster Plot", distance, clustering, 
                      sep = " "))
  
  # Make a plot of the cluster solution in the space specified 
  #   by the first two discriminant functions
  plotcluster(TD.cluster.norm, cuts, main="Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")
}
```

### Euclidean Distance and Complete Linkage
First, let's consider using a basic Euclidean distance metric along with complete linkage. We can see that this results in a well-structured dendrogram with distinct clusters and a relatively "bushy" tree. Now, considering the number of clusters to retain, we visually see either $3$ or $4$ distinct subtrees we could separate into. However, looking at the plot below, we see that the RSSTD statistic seems to take a local minimum value at around $3$, where the SPR and RS graphs also seem to flatten out. Upon trying both $3$ and $4$ clusters, $3$ seemed to provide a better degree of separation between groups in both principal component and discriminant function space, confirming $3$ groups as the correct choice.

Now, considering potential interpretations for these clusters, we see that Cluster 1 has a high proportion of teams who ended up with low seeds and a high number of tournament games played, implying that the characteristics of this group yield general tournament success. Cluster 2 is the largest group, encompassing teams with many different seeds and varying levels of tournament success. However, this group does contain the eventual champion Virginia, as well as the runner-up Texas Tech; thus, this group could also contain high achieving teams. Cluster 3 seems to contain mostly higher-seeded teams that only played a few tournament games, with notable standouts of Auburn and Purdue. However, being grouped with these teams would generally imply a lesser overall quality and hence lower chance of success.

```{r, echo = FALSE, message = FALSE}
cluster("euclidean", "complete", 3)
```

### Manhattan Distance and Complete Linkage
Now, to consider other distance metrics, let's try using Manhattan distance while keeping complete linkage agglomeration. In this case, we see a much more extented dendrogram in terms of greatest distance between clusters, though it visually has similar levels of bushiness. In terms of number of clusters to retain, this tree is much more ambiguous, as all the plotted metrics are very flat, with few extrema. From the tree structure, there seems to be $5$ significant subtrees (a cluster count where the RS plot seems to flatten out a bit as well), so we decide to keep $5$ clusters. Additionally, while the approximate confidence ellipses overlap significantly at any cluster count over $2$, the discriminant function plot retains a large level of separation for our choice of $5$ groups.

Compared to the Euclidean distance results, the groups here are quite different (which is obviously a result of additional groups as well). This Cluster 1 **(note: the group order does not matter)** contains some of the low seed teams, like North Carolina, as well as some of the highest performing teams, like Texas Tech. However, it also has some middle seed teams with only a game or two played, so it is not a surefire success group. The second group generally has high seed teams with few games played, with the notable exception of the Kentucky Wildcats. However, being in this group generally implies a lower level of success. Cluster 3 contains even more lowly teams, with mostly double-digit seeds and single games played (notably Fairleigh Dickinson played $2$ games but one was a play-in game). Cluster 4 has similar characteristics to Cluster 3 with slightly higher performance, even including eventual champion Virginia (implying Virginia's unique characteristics among successful teams). However, being in either of these groups generally implies a low level of team success (don't pick these teams for your bracket unless they are truly top seeds). On the other hand, Cluster 5 is comprised of generally successful teams with low seeds and many games played. 

```{r, echo = FALSE, message = FALSE}
cluster("manhattan", "complete", 5)
```

### Euclidean Distance and Single Linkage
In this case, we see a very "stringy, chain-like" dendrogram that is very sensitive to outliers. Each successive grouping seems to add only one team or a small subtree, so the number of clusters is relatively arbitrary (as evidenced by the flat plots up to high cluster numbers). Adding additional clusters similarly adds the next most "unique" team, generally those of highest and lowest seeds (with anything up to $5$ clusters, the individual teams are $1$ and $16$ seeds, with the seed numbers growing upwards and downwards, respectively, after that). Since choosing $8$ clusters depicts the greatest outliers, such as tournament favorite Duke and biggest underdog North Carolina Central, and several very successful teams, such as Michigan State and Auburn, we will retain $8$ clusters.

Now, while this method is useless in terms of creating larger groups with similar characteristics, it is actually incredibly useful for predicting tournament results. By picking out the outliers, it not only outlines pre-tournament favorites and "non-favorites" but also seems to predict outliers in terms of tournament success. Most notably, we see Auburn is among these $1$-team groups, one of the Cinderella or surprise teams who went far in the tournament despite having a relatively high seed. Thus this technique may be very useful for picking out successful teams (ignoring the abnormally "bad" teams and admitting a bias towards pre-tournament favorites).

```{r, echo = FALSE, message = FALSE}
cluster("euclidean", "single", 8)
```

## K-Means Cluster Analysis
While the hierarchial cluster analysis above was incredibly useful, it would also be very informative to look at a much greater pool of teams from multiple years. This is technically possible using the techniques above, but here we will do it in a much easier and quicker way using k-means cluster analysis. 

Below, using a modified version of the script by [Matt Peeples](https://github.com/mpeeples2008/Kmeans), we complete k-means cluster analysis on all the tournament teams with complete data from the 2014-15 to 2018-19 seasons. Looking at the two plots provided by this script, we see that the sum of squares vs. number of clusters flattens out at about $3$ (for log scale) or $4$ (for normal scale) clusters. Since the $4$ cluster solution is more interesting to interpret and still shows significant distinction on the principal component and discriminating function plots, we choose to retain $4$ clusters.

In this case, we see that Cluster 1 contains mostly middle seeds with mediocre tournament performance, with the exception of some "Cinderella" Final Four teams like 7 seed Michigan State in 2014-15, 10 seed Syracuse in 2015-16, and 7 seed South Carolina in 2016-17. Looking at the cluster means, we see that Cluster 1 has significantly lower adjusted averages for most offensive statistics (points, true shooting percentage, efficiency, etc.). However, their defensive statistics (opponent points, true shooting percentage, efficiency, etc.) are relatively average, with means of about $0$. Interestingly, while their adjusted winning percentage is expectedly low, their opponent winning percentage and strength of schedule are high, along with slightly above average RPI values. This implies that these teams are quality defensive teams with mediocre offenses who come from strong conferences. While their tournament performances are not stellar, membership in this group does imply the possibility of being a "Cinderella" team.

Cluster 2 teams generally seem to have lower seeds and better tournament performance, but there are still teams of all sorts. However, we see one champion in 2018-19 Virginia and three runner-ups in 2018-19 Texas Tech, 2017-18 Michigan, and 2014-15 Wisconsin, implying the possibility of strong performance. Despite this success, this cluster seems to have low offensive statistics, with the lowest points and efficiency. However, they have by far the strongest defensive statistics, with adjusted opponent points and efficiency means around $-1$. Finally, their composite statistics (adjusted winning percentage, opponent winning percentage, strength of schedule, RPI) are slightly below average, probably due to the large range of teams. Overall, the hallmark of this group is strong defense, which seemingly can allow for both outstanding and mediocre tournament performances.

Then, Cluster 3 is by far the lowest performing with the highest seeds, containing few exceptions. Fittingly, this is also the largest group, including many teams eliminated in the first two rounds. This cluster has generally mediocre offensive statistics along with poor defensive statistics. Furthermore, all of their composite statistics are significantly below average, implying both low performance and lowly competition. The only major exception comes from the 2018-19 Auburn team, which reached the Final Four, but this team is a known outlier from our previous clusters and thus does not seem to imply any silver lining for this low-performing group.
 
On the other hand, Cluster 4 is the highest performing and lowest-seed cluster, including the fewest early eliminations, two champions in 2015-16 and 2017-18 Villanova, and two runner-ups in 2015-16 North Carolina and 2016-17 Gonzaga. Likewise, these teams are the most well-rounded statistically, having very high offensive performance and generally strong defense as well. Notably, their composite statistics are also generally the highest, implying strong performance against good (if not always great) competition. If we were to pick a decisive "winner" group, this would be it.

Interestingly, these groups are all similarly-sized, perhaps reflecting some of the classic "randomness" of the tournament. But the existence of such clear and interpretable groups imply that there are significant patterns, even across multiple years (albeit with some expected "noise" in each cluster).

```{r k-means, echo = FALSE}
set.seed(07701)
# Create standardized data subset including all teams with complete data
TD.cluster.knorm <- scale(na.omit(TD.cluster[, vars.cluster.response]))
rownames(TD.cluster.knorm) <- paste(TD.cluster[, 1], "Seed:", TD.cluster[, "seed"], 
                             "Tourn Games Played:", TD.cluster[, "tourn_games_played"])

# kdata is just the normalized input dataset
kdata <- TD.cluster.knorm
# Set maximum value for number of clusters
n.lev <- 15

# Calculate the within groups sum of squared error (SSE) for 
#   the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets 
#   (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables 
#   and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual 
#   and randomized data
#   1st: Log scale
xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', 
     main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)
#   2nd: Normal scale
yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", 
     main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data 
#   and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets
#   1st: Log scale
xrange <- range(1:n.lev)
yrange <- range(log(r.sse.plus),log(r.sse.min))
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', 
     main='Cluster Solutions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), 
       lty=1)

#   2nd: Normal scale
xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', 
     main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), 
       lty=1)

clust.level <- 4

# Apply K-means cluster solutions - append clusters to CSV file
cat("Cluster Means")
fit <- kmeans(kdata, clust.level)
pander(aggregate(kdata, by=list(fit$cluster), FUN=mean), justify = "left")


team.clusters = list()
# Print a list of teams in each group, along with their seed 
#   and number of tournament games played
for (i in 1:clust.level)
{
  team.clusters[[i]] <- rownames(TD.cluster.knorm)[fit$cluster == i]
  print(paste("Teams in Cluster ", i))
  print(rownames(TD.cluster.knorm)[fit$cluster == i])
  print (" ")
}
  
test <- as.data.frame(sapply(team.clusters, "length<-", max(lengths(team.clusters))))
colnames(test) = c("Teams in Cluster 1", "Teams in Cluster 2", "Teams in Cluster 3", "Teams in Cluster 4")
#pander(test, justify = "left")

clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, TD.cluster.knorm)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified
clusplot(kdata, fit$cluster, shade=F, labels=1, lines=0, color=T, lty=4, cex=0.5, 
         main='Principal Components plot showing K-means clusters')


# Make plot of four cluster solution in space desginated by first two
#  two discriminant functions
plotcluster(kdata, fit$cluster, main="Four Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")
```